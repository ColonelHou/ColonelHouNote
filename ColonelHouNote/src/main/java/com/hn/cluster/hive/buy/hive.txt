安装：
1.下载tar包（http://apache.fayea.com/hive/hive-0.13.1/apache-hive-0.13.1-bin.tar.gz），解压安装
2.配置环境变量
export HIVE_HOME=/usr/local/hive
export PATH=$HIVE_HOME/bin:$PATH
source /etc/profile
echo $PATH | grep hive
$ hive
show tables显示ok就说明hive搭建好了
3.修改配置文件先加载hive-default.xml.template再加载hive-site.xml; 可以默认的,因为
<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>/opt/hadoop/hive/warehouse</value>
</property>
<property>
  <name>hive.querylog.location</name>
  <value>/opt/hadoop/hive/log</value>
</property>
hive元数据存储：
Derby：
   单session， 
   一个终端下创建的表，在另一个终端下启动是无法看到的。
   不能多用户共享
MySQL
   安装，拷贝mysql-connector-java-xxx-bin.jar到hive的lib目录下，这样可以多用户共享
   修改hive-site.xml中四个属性
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=latin1</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>root</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>root</value>
</property>
4.hive访问方式：
  ・Cli
  ・Hwi    hive --service hwi   /http://localhost:9999/hwi
  ・HiveServer  hive --service hiveserver & 后台运行  jdbc来访问
========================================问题===============================
http://blog.csdn.net/wulantian/article/details/38271803
cd apache-hive-0.13.1-src
cd hwi/web
zip hive-hwi-0.13.1.zip ./*     //打包成.zip文件。
*/
scp hive-hwi-0.13.1.war db96:/usr/local/hive/lib/   //放到hive的安装目录的lib目录下。
修改配置文件：hive-site.xml 增加如下三个参数项：
<property>
<name>hive.hwi.listen.host</name>
<value>0.0.0.0</value>
</property>
<property>
<name>hive.hwi.listen.port</name>
<value>9999</value>
</property>
<property>
<name>hive.hwi.war.file</name>
<value>lib/hive-hwi-0.13.1.war</value>   
</property>
保存退出即可。
启动hwi:
Problem accessing /hwi/. Reason:

    Unable to find a javac compiler;
com.sun.tools.javac.Main is not on the classpath.
Perhaps JAVA_HOME does not point to the JDK.
It is currently set to "/usr/local/jdk/jre"
Caused by:

Unable to find a javac compiler;
com.sun.tools.javac.Main is not on the classpath.
Perhaps JAVA_HOME does not point to the JDK.
It is currently set to "/usr/local/jdk/jre"
	at org.apache.tools.ant.taskdefs.compilers.CompilerAdapterFactory.getCompiler(CompilerAdapterFactory.java:129)
	at org.apache.tools.ant.taskdefs.Javac.findSupportedFileExtensions(Javac.java:979)
	at org.apache.tools.ant.taskdefs.Javac.scanDir(Javac.java:956)
	at org.apache.tools.ant.taskdefs.Javac.execute(Javac.java:927)
	at org.apache.jasper.compiler.AntCompiler.generateClass(AntCompiler.java:220)
	at org.apache.jasper.compiler.Compiler.compile(Compiler.java:298)
cp /usr/java/jdk1.7.0_55/lib/tools.jar  /usr/local/hive/lib/
=================================问题=================
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes
sudo chown mysql:mysql /usr/local/mysql/data/ -R
mysql> alter database hive character set latin1;

------------使用hive的时候hadoop的安全模式要关闭------------------------
[hadoop@hadoopMaster ~]$hdfs dfsadmin -safemode get
set hive.cli.print.current.db=true;
set hive.cli.print.header=true;
Oracle VM 网卡一，网卡2仅主机  / 自带的connect Server

RCFile把一列数据转换成一行
桶Buckets的概念， 为了提高查询效率
create [external] table test(name string comment 'name value', addr string comment 'addr value') row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;
create table test2 like test;
create table test3 as selsect name, addr from test2;#走的是MR,含数据
desc[extended|formatted] test;
外部表与内部表区别：Table Type不一样， External_TABLE/Manage_table
load data [local] inpath '/home/hadoop/data/log.txt' overwrite into table test;
drop table test;
--------------Hive不同文件读取对比-------------------
・stored as textfile   		直接查看HDFS  hdfs dfs -text
・stored as sequencefile     hdfs dfs -text
・stored as rcfile   		hive --service rdfilecat path
・sotred as inputformat 'class'  outformat 'class'
create table test_text(name string) stored as textfile;
create table test_seq(name string) stored as sequencefile;
create table test_rc(name string) stored as rcfile;
create table test_class(name string) stored as 'selfDefineClassFormat';
hive> add jar /home/hadoop/my.jar; #添加到hive路经下hive/lib下
================Hive使用SerDe序列化与反序列化==============
读写数据顺序如下：
HDFS文件――》InputFileFormat――》<k,v>
org.apache.hadoop.hive.serde2.SerDe
org.apache.hadoop.hive.contrib.serde2.RegexSerDe;
row format SERDE '' with SERDEPROPERTIES("input.regex"="([^ ]*) ([^ ]*)... ([0-9]*)..")
================HIVE分区表===================================
・分区
  在HIVE SELECT查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作
  分区表指的是在创建表时指定partition的分区空间
create table tblPartition(name string)partitioned by(key type,...)row for
  eg:一天一个分区， 指定查询那天的数据，就会到指定的分区目录下查询，
alter table tbl add|drop if not exists partition(country='xxx'[, state='yyy'])
alter table test_partition add|drop if not exists partition(country='2014'])
create table test_partition(name string) partitioned by(dt string);
ALTER TABLE test_partition ADD PARTITION (dt='2008-08-08');
hive>show partitions tbl;
==================HIVE分桶============================
・分桶
  对于每一个表或者分区， HIVE可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分
  HIVE是针对某一列进行分桶
  HIVE采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶中
・好处
  获得更高的查询处更效率
  使取样更高效
create table bucketed_user(id string, name string) clustered by(id) sorted by(name) into 4 buckets row format delimited fields terminated by '\t' stored as textfile;按id化分，按name进行排序ASC,分四个桶
12	tom
34	jim
45	jily
78	john
set hive.enforce.bucketing=true;
insert into bucketed_user select name,
=======================HIVE数据操作======================
hive执行命令方式：cli, jdbc, hwi, beeline(shell里执行JDBC)
hive --service -help
[hadoop@hadoopMaster ~]$hive -S -e 'SELECT * from struct_test;' > /opt/trace/tmp/hive/test.test  #加-S  -f 后面加的是HQL文件
hive>list jar;可以显示中分布式中缓存的jar， 比如hive>add jar xxx.jar;
hive>source /opt/hive/xxx.hql执行
----------------------------------------------------------------------
hive> set val=wer;
hive> set val;
val=wer
hive> select * from testte where name='${hiveconf:val}';
----------------------------------------------------------------------
[hadoop@hadoopMaster ]$ env 打印环境变量
------------------------------HIVE中执行shell命令------------------
hive>dfs -ls -R /;
hive> !ls /opt;
hadoop
rh
trace
from test insert overwrite table test_m select name, id where name = 'xx';
=============================HIVE数据导出=========================
・hadoop命令方式
  hive> dfs -get /opt/hadoop/hive/warehouse/struct_test/struct_test.data /opt/trace/tmp/hive/hadoopExport.data
  hive> dfs -text /opt/hadoop/hive/warehouse/struct_test/struct_test.data;
  1,jim:20
  2,lily:30
  3,lucy:25
  4,john:34

・通过INSERT...DIRECTORY方式
  hive> insert overwrite [local] directory '/opt/trace/tmp/hive/insert' row format delimited fields terminated by ',' select * from struct_test;

・Shell命令加管道  hive -f/e | sed/grep/awk  > file
・第三方工具 sqoop
==========================HIVE动态分区==============================
・动态分区
  不需要为不同的分区添加不同的插入语句
  分区不确定，需要 从数据中获取
几个参数：
  set hive.exec.dynamic.partition=true;
  //使用动态分区
  set hive.exec.dynamic.partition.mode=nonstrick;
  //无限制模式 ，如果模式是strict，则必须有一个静态分区，且放在最前面
  如果我有三个分区，必须有一个静态分区，其它二个
  set hive.exec.max.dynamic.partitions.pernode=10000;
  //每个节点生成 动态分区的最大个数
  set hive.exec.max.dynamic.partitions=100000;
  //生成动态分区的最大个数
  set hive.exec.max.created.files=150000;
  //一个任务最多可以创建的文件数目
  set dfs.datanode.max.xcievers=8192;
  //限定一次最多打开的文件数
 create table d_part(name string)partitioned by (value string) row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;
 create table dd_part(name string, id string)row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;
 wer     46
 wer     89
 weree   78
 rr      89
 load data local inpath '/opt/trace/tmp/hive/partition/d_part.data' overwrite into table dd_part;
 insert overwrite table d_part partition(value)select name, id as value from dd_part;
=========================表属性操作
修改表名：alter table tblName rename to newTblName;
修改列名：alter table tblName change column oldCol newCol int comment 'xxx';
增加列：alter table tblname add columns(c1 string comment 'xxx', c2 long comment 'yyyy');
hive> alter table testch add columns(type string, col int comment 'xxx');
OK
Time taken: 0.24 seconds
hive> desc testch;
OK
name                    string
value                   string
type                    string
col                     int                     xxx
hive> alter table testch change column type type string [after name]/first;
type放到name的后面
========================HIVE高级查询===========================
查询操作group by , order by(全局排序), join, distribute by(分散), sort by(每个reduce上排序), cluster by(按某个key聚合并排序), union all
底层实现：MR
count计数：count(*):都不为空才加1, count(1)不管是否为空都加1, count(col):值为非null才加1
sum求和：sum(可转成数字的值)返回bigint
avg求平均值：avg(可转成数字的值)返回double
执行
distinct不同值个数：count(distinct col)
order by只有一个reduce来执行，跟配置无关； 当数据量超大，会搞到一个Reduce上执行。
create table MM(col string, col2 string)row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;
A	1
C	5
B	2
C	3
load data local inpath '/opt/trace/tmp/hive/orderby.data' overwrite into table MM;
desc M;
hive> select * from MM order by col desc, col2 asc;
Total MapReduce CPU Time Spent: 3 seconds 990 msec
OK
C       3
C       5
B       2
A       1
order by执行过程：
从表读取数据，执行where条件，在MAP阶段； 
------------------group by-------------------------------------
注意：
・select后面非聚合列必须出现在group by中
・除了普通列就是一些聚合操作
・group by后面也可以跟表达式，比如substr(col)
where条件是在map中执行，group by是放到reduce中执行；
特性：使用了Reduce操作，受限于reduce数量，设置reduce参数mapred.reduce.tasks，输出文件个数与reduce数相同，文件大小与reduce处理的数据量有关
・问题：
  网络负载过重
  数据倾斜，优化参数hive.groupby.skewindata=true;
hive> set mapred.reduce.tasks=5;对于order by不启作用，但是对group by起作用,教程中会启二个job来执行的； 
=========================join操作================================
co1 col2
1	w
3	r
5	e
-----b
co3 col4
1	f
1	g
4	j
(select col from a)s join (select col from b)t on s.col=t.col3;
left outer|right outer|left semi| 
set hive.optimize.skewjoin=true; #优化操作，存在数据倾斜就设置他
执行过程：
第一步：map读取数据，执行where条件――>分组，传到reduce端； 
--------------------MAPJOIN------------------------------
  在map端把小表加载到内存中，每个计算结点都是； 然后读取压而不服，和内存中小表完成连接操作，
  其中使用了分布式缓存技术
 优缺点：
   不消耗集群的Reduce资源
   减少Reducer操作，加快程序执行
   降低网络负载
   占用部分内存，所以加载到内存中的表不能过大，因为每个计算节点都会加载一次
   生成较多的小文件
  配置以下参数，是hive自动根据sql，选择使用common join或者map join
    set hive.auto.convert.join=true;
    hive.mapjoin.smalltable.filesize默认是25MB
  第二种手动指定
    select /*+mapjoin(n)*/ m.col,...from b join n on xxx = yyy;
	使用场景：
	1.关联操作中有一张表非常小
	2.不等值的链接操作
HIVE分桶使用：
  select * from bucketed_user tbl(bucket 1 out 2 on id)
 bucket join 
   set hive.optimize.bucketmapjoin=true;
   set hive.optimize.bucketmapjoin.sortedmerge=true;
   set hive.input.format=org.apache.hadoop.hive.hive.ql.io.BucketizedHiveInputFormat;
===============================Distribute by 和 Sort by================
Distribute分散数据
  distribute by col...按照col..列把数据分散到不同的reduce
Sort排序：
  sort by col2..
  按照col2..列把数据排序
select col1,col2 from M distribute by col1 sort by col1 asc, col2 desc;
两者结合出现，确保每个reduce的输出都是有序的； 
-----------------distribute by与group by对比------------
  都是按key值划分数据
  都使用reduce操作
  唯一不同，distribute by只是单纯分散数据，而group by把相同key数据聚焦到一起，后续必须是聚合操作
------------------order by与sort by-----------------
 order by是全局排序
 sort by只是确保每个reduce上面输出的数据有序,如果只有一个reduce时，和orderby作用一样
------------应用场景-----------
map输出的文件大小不均
reduce输出的文件大小不均
小文件过多
文件超大
=============================cluster by==========================
把有相同值的数据聚焦到一起，并排序
效果：
  cluster by col
  distribute by col order by col;
----------
http://www.cnblogs.com/ggjucheng/archive/2013/01/03/2843243.html
==============================union all==========================
只有map操作就把两个张合并成一张表